// Package worker æä¾›é«˜æ€§èƒ½çš„å¼‚æ­¥ä»»åŠ¡å¤„ç†åŠŸèƒ½
// æ”¯æŒå¹¶å‘å¤„ç†ã€è´Ÿè½½å‡è¡¡å’Œé”™è¯¯æ¢å¤
package worker

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"runtime"
	"sync"
	"time"

	"icpt-system/internal/config"
	"icpt-system/internal/store"

	"github.com/go-redis/redis/v8"
)

// HighPerformanceWorker é«˜æ€§èƒ½å·¥ä½œå™¨
type HighPerformanceWorker struct {
	redisClient *redis.Client
	workerCount int
	ctx         context.Context
	cancel      context.CancelFunc
	wg          sync.WaitGroup
	stats       *WorkerStats
}

// WorkerStats å·¥ä½œå™¨ç»Ÿè®¡ä¿¡æ¯
type WorkerStats struct {
	TotalProcessed   int64         // æ€»å¤„ç†æ•°
	SuccessCount     int64         // æˆåŠŸæ•°
	FailureCount     int64         // å¤±è´¥æ•°
	AverageLatency   time.Duration // å¹³å‡å»¶è¿Ÿ
	CurrentQueueSize int64         // å½“å‰é˜Ÿåˆ—å¤§å°
	mu               sync.RWMutex
}

// NewHighPerformanceWorker åˆ›å»ºé«˜æ€§èƒ½å·¥ä½œå™¨
func NewHighPerformanceWorker() *HighPerformanceWorker {
	ctx, cancel := context.WithCancel(context.Background())

	workerCount := config.Cfg.Performance.WorkerCount
	if workerCount <= 0 {
		workerCount = runtime.NumCPU() // é»˜è®¤ä½¿ç”¨CPUæ ¸å¿ƒæ•°
	}

	return &HighPerformanceWorker{
		redisClient: store.RedisClient,
		workerCount: workerCount,
		ctx:         ctx,
		cancel:      cancel,
		stats:       &WorkerStats{},
	}
}

// Start å¯åŠ¨é«˜æ€§èƒ½å·¥ä½œå™¨
func (w *HighPerformanceWorker) Start() {
	log.Printf("ğŸš€ å¯åŠ¨é«˜æ€§èƒ½Workerï¼Œå¹¶å‘æ•°: %d", w.workerCount)

	// å¯åŠ¨å¤šä¸ªå·¥ä½œåç¨‹
	for i := 0; i < w.workerCount; i++ {
		w.wg.Add(1)
		go w.workerRoutine(i)
	}

	// å¯åŠ¨ç»Ÿè®¡åç¨‹
	w.wg.Add(1)
	go w.statsRoutine()

	log.Println("âœ… é«˜æ€§èƒ½Workerå¯åŠ¨æˆåŠŸ")
}

// Stop åœæ­¢å·¥ä½œå™¨
func (w *HighPerformanceWorker) Stop() {
	log.Println("ğŸ›‘ åœæ­¢é«˜æ€§èƒ½Worker...")
	w.cancel()
	w.wg.Wait()
	log.Println("âœ… é«˜æ€§èƒ½Workerå·²åœæ­¢")
}

// workerRoutine å·¥ä½œåç¨‹
func (w *HighPerformanceWorker) workerRoutine(workerID int) {
	defer w.wg.Done()

	log.Printf("Worker-%d å¯åŠ¨", workerID)

	for {
		select {
		case <-w.ctx.Done():
			log.Printf("Worker-%d æ”¶åˆ°åœæ­¢ä¿¡å·", workerID)
			return
		default:
			w.processNextTask(workerID)
		}
	}
}

// processNextTask å¤„ç†ä¸‹ä¸€ä¸ªä»»åŠ¡
func (w *HighPerformanceWorker) processNextTask(workerID int) {
	startTime := time.Now()

	// ä»Redisé˜Ÿåˆ—è·å–ä»»åŠ¡ï¼ˆé˜»å¡å¼ï¼‰
	result, err := w.redisClient.BLPop(w.ctx, 1*time.Second, "image_processing_queue").Result()
	if err != nil {
		if err != redis.Nil && err != context.Canceled {
			log.Printf("Worker-%d è·å–ä»»åŠ¡å¤±è´¥: %v", workerID, err)
		}
		return
	}

	// è§£æä»»åŠ¡æ•°æ®
	taskData := result[1]
	var task map[string]interface{}
	if err := json.Unmarshal([]byte(taskData), &task); err != nil {
		log.Printf("Worker-%d è§£æä»»åŠ¡å¤±è´¥: %v", workerID, err)
		w.updateStats(false, time.Since(startTime))
		return
	}

	// å¤„ç†ä»»åŠ¡
	success := w.handleImageProcessingTask(workerID, task)
	latency := time.Since(startTime)

	// æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
	w.updateStats(success, latency)

	if success {
		log.Printf("Worker-%d ä»»åŠ¡å¤„ç†æˆåŠŸï¼Œè€—æ—¶: %v", workerID, latency)
	} else {
		log.Printf("Worker-%d ä»»åŠ¡å¤„ç†å¤±è´¥ï¼Œè€—æ—¶: %v", workerID, latency)
	}
}

// handleImageProcessingTask å¤„ç†å›¾åƒå¤„ç†ä»»åŠ¡
func (w *HighPerformanceWorker) handleImageProcessingTask(workerID int, task map[string]interface{}) bool {
	// è¿™é‡Œå¤ç”¨ç°æœ‰çš„å›¾åƒå¤„ç†é€»è¾‘
	// ä½†æ˜¯ä¼šé’ˆå¯¹æ€§èƒ½è¿›è¡Œä¼˜åŒ–

	imageID, ok := task["imageId"].(float64)
	if !ok {
		log.Printf("Worker-%d æ— æ•ˆçš„imageId", workerID)
		return false
	}

	userID, ok := task["userId"].(float64)
	if !ok {
		log.Printf("Worker-%d æ— æ•ˆçš„userId", workerID)
		return false
	}

	originalFilename, ok := task["originalFilename"].(string)
	if !ok {
		log.Printf("Worker-%d æ— æ•ˆçš„originalFilename", workerID)
		return false
	}

	storagePath, ok := task["storagePath"].(string)
	if !ok {
		log.Printf("Worker-%d æ— æ•ˆçš„storagePath", workerID)
		return false
	}

	log.Printf("Worker-%d å¼€å§‹å¤„ç†å›¾åƒ: ID=%d, æ–‡ä»¶=%s", workerID, int(imageID), originalFilename)

	// æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
	if err := updateImageStatus(uint(imageID), "processing", ""); err != nil {
		log.Printf("Worker-%d æ›´æ–°çŠ¶æ€å¤±è´¥: %v", workerID, err)
		return false
	}

	// ç”Ÿæˆç¼©ç•¥å›¾ï¼ˆè¿™é‡Œå¯ä»¥è°ƒç”¨C++å¤„ç†æ¨¡å—ï¼‰
	thumbnailPath, err := w.generateThumbnail(storagePath, originalFilename)
	if err != nil {
		log.Printf("Worker-%d ç”Ÿæˆç¼©ç•¥å›¾å¤±è´¥: %v", workerID, err)
		updateImageStatus(uint(imageID), "error", err.Error())
		return false
	}

	// æ›´æ–°æ•°æ®åº“ï¼Œæ ‡è®°ä¸ºå®Œæˆ
	if err := updateImageCompletion(uint(imageID), thumbnailPath); err != nil {
		log.Printf("Worker-%d æ›´æ–°å®ŒæˆçŠ¶æ€å¤±è´¥: %v", workerID, err)
		return false
	}

	return true
}

// generateThumbnail ç”Ÿæˆç¼©ç•¥å›¾ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
func (w *HighPerformanceWorker) generateThumbnail(sourcePath, filename string) (string, error) {
	// è¿™é‡Œå¯ä»¥é›†æˆæ›´é«˜æ€§èƒ½çš„å›¾åƒå¤„ç†åº“
	// æˆ–è€…è°ƒç”¨C++æ¨¡å—

	outputPath := fmt.Sprintf("uploads/thumbnails/thumb_%s", filename)

	// æ¨¡æ‹Ÿå›¾åƒå¤„ç†ï¼ˆå®é™…åº”è¯¥è°ƒç”¨çœŸå®çš„å¤„ç†å‡½æ•°ï¼‰
	// åœ¨é«˜å¹¶å‘æƒ…å†µä¸‹ï¼Œè¿™é‡Œå¯ä»¥ä½¿ç”¨æ›´é«˜æ•ˆçš„å›¾åƒå¤„ç†ç®—æ³•

	// æ¨¡æ‹Ÿå¤„ç†æ—¶é—´ï¼ˆå®é™…å¤„ç†ä¼šæ›´å¿«ï¼‰
	time.Sleep(50 * time.Millisecond)

	return outputPath, nil
}

// updateStats æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
func (w *HighPerformanceWorker) updateStats(success bool, latency time.Duration) {
	w.stats.mu.Lock()
	defer w.stats.mu.Unlock()

	w.stats.TotalProcessed++
	if success {
		w.stats.SuccessCount++
	} else {
		w.stats.FailureCount++
	}

	// è®¡ç®—å¹³å‡å»¶è¿Ÿ
	if w.stats.TotalProcessed == 1 {
		w.stats.AverageLatency = latency
	} else {
		totalLatency := time.Duration(w.stats.TotalProcessed-1)*w.stats.AverageLatency + latency
		w.stats.AverageLatency = totalLatency / time.Duration(w.stats.TotalProcessed)
	}
}

// statsRoutine ç»Ÿè®¡åç¨‹
func (w *HighPerformanceWorker) statsRoutine() {
	defer w.wg.Done()

	ticker := time.NewTicker(30 * time.Second)
	defer ticker.Stop()

	for {
		select {
		case <-w.ctx.Done():
			return
		case <-ticker.C:
			w.printStats()
		}
	}
}

// printStats æ‰“å°ç»Ÿè®¡ä¿¡æ¯
func (w *HighPerformanceWorker) printStats() {
	w.stats.mu.RLock()
	defer w.stats.mu.RUnlock()

	// è·å–å½“å‰é˜Ÿåˆ—å¤§å°
	queueSize, _ := w.redisClient.LLen(w.ctx, "image_processing_queue").Result()
	w.stats.CurrentQueueSize = queueSize

	successRate := float64(0)
	if w.stats.TotalProcessed > 0 {
		successRate = float64(w.stats.SuccessCount) / float64(w.stats.TotalProcessed) * 100
	}

	log.Printf("ğŸ“Š Workerç»Ÿè®¡ä¿¡æ¯:")
	log.Printf("  æ€»å¤„ç†æ•°: %d", w.stats.TotalProcessed)
	log.Printf("  æˆåŠŸæ•°: %d", w.stats.SuccessCount)
	log.Printf("  å¤±è´¥æ•°: %d", w.stats.FailureCount)
	log.Printf("  æˆåŠŸç‡: %.1f%%", successRate)
	log.Printf("  å¹³å‡å»¶è¿Ÿ: %v", w.stats.AverageLatency)
	log.Printf("  å½“å‰é˜Ÿåˆ—: %d", w.stats.CurrentQueueSize)
	log.Printf("  Workeræ•°: %d", w.workerCount)
}

// GetStats è·å–ç»Ÿè®¡ä¿¡æ¯
func (w *HighPerformanceWorker) GetStats() *WorkerStats {
	w.stats.mu.RLock()
	defer w.stats.mu.RUnlock()

	// å¤åˆ¶ç»Ÿè®¡ä¿¡æ¯ä»¥é¿å…å¹¶å‘é—®é¢˜
	statsCopy := *w.stats

	// è·å–å½“å‰é˜Ÿåˆ—å¤§å°
	queueSize, _ := w.redisClient.LLen(w.ctx, "image_processing_queue").Result()
	statsCopy.CurrentQueueSize = queueSize

	return &statsCopy
}

// è¾…åŠ©å‡½æ•°ï¼ˆå¤ç”¨ç°æœ‰ä»£ç ï¼‰
func updateImageStatus(imageID uint, status, errorInfo string) error {
	// è¿™é‡Œè°ƒç”¨ç°æœ‰çš„æ•°æ®åº“æ›´æ–°å‡½æ•°
	return store.UpdateImageStatus(imageID, status, errorInfo)
}

func updateImageCompletion(imageID uint, thumbnailPath string) error {
	// è¿™é‡Œè°ƒç”¨ç°æœ‰çš„æ•°æ®åº“æ›´æ–°å‡½æ•°
	return store.UpdateImageCompletion(imageID, thumbnailPath)
}
